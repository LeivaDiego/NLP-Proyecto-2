{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445283ea",
   "metadata": {},
   "source": [
    "# Proyecto 2\n",
    "# Procesamiento de Lenguaje Natural basados en arquitecturas neuronales secuenciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b41ac9",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ac7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b4238",
   "metadata": {},
   "source": [
    "## Preparación del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c8f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto bruto: 3975\n",
      "Érase un pobre campesino que estaba una noche junto al hogar atizando el fuego, mientras su mujer hilaba, sentada a su lado.\n",
      "\n",
      "Dijo el hombre: – ¡Qué triste es no tener hijos! ¡Qué silencio en esta casa, mientras en las otras todo es ruido y alegría! – Sí -respondió la mujer, suspirando-. Aunque fuese uno solo, y aunque fuese pequeño como el pulgar, me daría por satisfecha. Lo querríamos más que nuestra vida.\n",
      "\n",
      "Sucedió que la mujer se sintió descompuesta, y al cabo de siete meses trajo al mundo un\n"
     ]
    }
   ],
   "source": [
    "# Descargar el modelo de spaCy para español si no está ya descargado\n",
    "# python -m spacy download es_core_news_sm\n",
    "\n",
    "# Cargar el modelo de spaCy para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Rutas\n",
    "RAW_PATH = \"Data/raw_corpus.txt\"          # entrada\n",
    "OUT_DIR = \"Data/processed\"                # carpeta de salida\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Cargar el texto bruto\n",
    "with open(RAW_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Longitud del texto bruto:\", len(raw_text))\n",
    "print(raw_text[:500])  # muestra un fragmento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004a6c5",
   "metadata": {},
   "source": [
    "### Limpieza básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2b975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto limpio: 3975\n",
      "Érase un pobre campesino que estaba una noche junto al hogar atizando el fuego, mientras su mujer hilaba, sentada a su lado.\n",
      "\n",
      "Dijo el hombre: – ¡Qué triste es no tener hijos! ¡Qué silencio en esta casa, mientras en las otras todo es ruido y alegría! – Sí -respondió la mujer, suspirando-. Aunque fuese uno solo, y aunque fuese pequeño como el pulgar, me daría por satisfecha. Lo querríamos más que nuestra vida.\n",
      "\n",
      "Sucedió que la mujer se sintió descompuesta, y al cabo de siete meses trajo al mundo un\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Normalizar saltos de línea: quitar espacios antes de saltos\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "\n",
    "    # Normalizar comillas curvas a comillas rectas (opcional)\n",
    "    text = text.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    \n",
    "    # Unificar guiones largos de diálogo (si aparecen variantes)\n",
    "    text = text.replace(\"—\", \"–\")\n",
    "    \n",
    "    # Quitar espacios repetidos\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    \n",
    "    # Quitar saltos de línea múltiples excesivos (más de 2)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    \n",
    "    # Strip global\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "clean_corpus = clean_text(raw_text)\n",
    "\n",
    "print(\"Longitud del texto limpio:\", len(clean_corpus))\n",
    "print(clean_corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51174b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto limpio guardado en: Data/processed/clean_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "# Guardar el texto limpio\n",
    "clean_corpus_path = os.path.join(OUT_DIR, \"clean_corpus.txt\").replace(\"\\\\\", \"/\")\n",
    "with open(clean_corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_corpus)\n",
    "\n",
    "print(f\"Texto limpio guardado en: {clean_corpus_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50276c0",
   "metadata": {},
   "source": [
    "### Tokenizacion y Segmentacion en oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4f2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de oraciones detectadas: 41\n",
      "\n",
      "Oración 0: Érase un pobre campesino que estaba una noche junto al hogar atizando el fuego, mientras su mujer hilaba, sentada a su lado.\n",
      "Tokens: ['Érase', 'un', 'pobre', 'campesino', 'que', 'estaba', 'una', 'noche', 'junto', 'al', 'hogar', 'atizando', 'el', 'fuego', ',', 'mientras', 'su', 'mujer', 'hilaba', ',', 'sentada', 'a', 'su', 'lado', '.']\n",
      "\n",
      "Oración 1: Dijo el hombre: – ¡Qué triste es no tener hijos! ¡Qué silencio en esta casa, mientras en las otras todo es ruido y alegría! –\n",
      "Tokens: ['Dijo', 'el', 'hombre', ':', '–', '¡', 'Qué', 'triste', 'es', 'no', 'tener', 'hijos', '!', '¡', 'Qué', 'silencio', 'en', 'esta', 'casa', ',', 'mientras', 'en', 'las', 'otras', 'todo', 'es', 'ruido', 'y', 'alegría', '!', '–']\n",
      "\n",
      "Oración 2: Sí -respondió la mujer, suspirando-.\n",
      "Tokens: ['Sí', '-respondió', 'la', 'mujer', ',', 'suspirando-', '.']\n",
      "\n",
      "Oración 3: Aunque fuese uno solo, y aunque fuese pequeño como el pulgar, me daría por satisfecha.\n",
      "Tokens: ['Aunque', 'fuese', 'uno', 'solo', ',', 'y', 'aunque', 'fuese', 'pequeño', 'como', 'el', 'pulgar', ',', 'me', 'daría', 'por', 'satisfecha', '.']\n",
      "\n",
      "Oración 4: Lo querríamos más que nuestra vida.\n",
      "\n",
      "Sucedió que la mujer se sintió descompuesta, y al cabo de siete meses trajo al mundo un niño que, si bien perfectamente conformado en todos sus miembros, no era más largo que un dedo pulgar.\n",
      "Tokens: ['Lo', 'querríamos', 'más', 'que', 'nuestra', 'vida', '.', 'Sucedió', 'que', 'la', 'mujer', 'se', 'sintió', 'descompuesta', ',', 'y', 'al', 'cabo', 'de', 'siete', 'meses', 'trajo', 'al', 'mundo', 'un', 'niño', 'que', ',', 'si', 'bien', 'perfectamente', 'conformado', 'en', 'todos', 'sus', 'miembros', ',', 'no', 'era', 'más', 'largo', 'que', 'un', 'dedo', 'pulgar', '.']\n"
     ]
    }
   ],
   "source": [
    "# Procesar todo el corpus con spaCy\n",
    "doc = nlp(clean_corpus)\n",
    "\n",
    "sentences = []\n",
    "tokens_per_sentence = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sent_text = sent.text.strip()\n",
    "    if not sent_text:\n",
    "        continue\n",
    "    \n",
    "    # Lista de tokens de la oración, sin espacios\n",
    "    toks = [tok.text for tok in sent if not tok.is_space]\n",
    "    \n",
    "    # Filtramos oraciones muy cortas si quieres (opcional)\n",
    "    if len(toks) == 0:\n",
    "        continue\n",
    "    \n",
    "    sentences.append(sent_text)\n",
    "    tokens_per_sentence.append(toks)\n",
    "\n",
    "print(\"Número de oraciones detectadas:\", len(sentences))\n",
    "for i in range(min(5, len(sentences))):\n",
    "    print(f\"\\nOración {i}: {sentences[i]}\")\n",
    "    print(\"Tokens:\", tokens_per_sentence[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3efcd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos tokenizados guardados en: Data/processed/sentences_tokenized_spacy.csv\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences_str = [\" \".join(toks) for toks in tokens_per_sentence]\n",
    "\n",
    "data_df = pd.DataFrame({\n",
    "    \"id\": list(range(len(sentences))),\n",
    "    \"sentence\": sentences,\n",
    "    \"tokens\": tokenized_sentences_str\n",
    "})\n",
    "\n",
    "all_sentences_path = os.path.join(OUT_DIR, \"sentences_tokenized_spacy.csv\").replace(\"\\\\\", \"/\")\n",
    "data_df.to_csv(all_sentences_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Datos tokenizados guardados en: {all_sentences_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2a4f6",
   "metadata": {},
   "source": [
    "### División de entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d51ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaños:\n",
      "train: 32\n",
      "val:   4\n",
      "test:  5\n",
      "\n",
      "Conjuntos guardados en:\n",
      "- Train: Data/processed/train_sentences_spacy.csv\n",
      "- Validation: Data/processed/val_sentences_spacy.csv\n",
      "- Test: Data/processed/test_sentences_spacy.csv\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(len(data_df)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "n_total = len(indices)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val = int(0.1 * n_total)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "val_idx = indices[n_train:n_train + n_val]\n",
    "test_idx = indices[n_train + n_val:]\n",
    "\n",
    "train_df = data_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = data_df.iloc[val_idx].reset_index(drop=True)\n",
    "test_df = data_df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Tamaños:\")\n",
    "print(\"train:\", len(train_df))\n",
    "print(\"val:  \", len(val_df))\n",
    "print(\"test: \", len(test_df))\n",
    "\n",
    "train_path = os.path.join(OUT_DIR, \"train_sentences_spacy.csv\").replace(\"\\\\\", \"/\")\n",
    "val_path = os.path.join(OUT_DIR, \"val_sentences_spacy.csv\").replace(\"\\\\\", \"/\")\n",
    "test_path = os.path.join(OUT_DIR, \"test_sentences_spacy.csv\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False, encoding=\"utf-8\")\n",
    "val_df.to_csv(val_path, index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(test_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nConjuntos guardados en:\")\n",
    "print(\"- Train:\", train_path)\n",
    "print(\"- Validation:\", val_path)\n",
    "print(\"- Test:\", test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
